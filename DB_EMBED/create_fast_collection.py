"""Create a FAST working collection with top 100K documents for 16GB RAM"""

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, Filter, FieldCondition, Range
import time

print("=" * 70)
print("üöÄ Creating Fast Working Collection (100K docs)")
print("   Perfect for 16GB RAM - Will be INSTANT searches!")
print("=" * 70)

client = QdrantClient(url="http://localhost:6333", timeout=300)

old_collection = "vietnamese_legal_content"
new_collection = "vietnamese_legal_fast"

# Step 1: Create new optimized collection
print("\n1. Creating new fast collection...")
try:
    client.recreate_collection(
        collection_name=new_collection,
        vectors_config=VectorParams(
            size=768, distance=Distance.COSINE, on_disk=False  # Keep in RAM for speed
        ),
        optimizers_config={
            "indexing_threshold": 5000,
            "memmap_threshold": 0,  # No memory mapping for small collection
        },
        hnsw_config={
            "m": 32,  # Higher quality for smaller collection
            "ef_construct": 200,
            "full_scan_threshold": 5000,
        },
    )
    print("‚úÖ New collection created")
except Exception as e:
    print(f"Error: {e}")
    exit(1)

# Step 2: Copy top 100K documents with longest content
print("\n2. Copying top 100K documents with longest content...")
print("   Strategy: Get documents with content_length > 3000 chars")
print("   This gives you the highest quality legal documents")

copied = 0
batch_size = 100
scroll_offset = None
target_count = 100000

print("\n   Progress:")

try:
    while copied < target_count:
        # Scroll through collection with filter
        result = client.scroll(
            collection_name=old_collection,
            scroll_filter=Filter(
                must=[
                    FieldCondition(
                        key="content_length",
                        range=Range(gte=3000),  # Only long, quality documents
                    )
                ]
            ),
            limit=batch_size,
            offset=scroll_offset,
            with_payload=True,
            with_vectors=True,
        )

        points = result[0]
        scroll_offset = result[1]

        if not points:
            print(f"\n   Reached end at {copied} documents")
            break

        # Upload to new collection
        client.upsert(collection_name=new_collection, points=points, wait=False)

        copied += len(points)

        if copied % 1000 == 0:
            print(f"   {copied:,} / {target_count:,} ({copied/target_count*100:.1f}%)")

        if copied >= target_count:
            break

except Exception as e:
    print(f"\n   ‚ùå Error during copy: {e}")
    print(f"   But we got {copied:,} documents - that might be enough!")

print(f"\n‚úÖ Copied {copied:,} high-quality documents")

# Step 3: Get stats
print("\n3. Verifying new collection...")
time.sleep(2)  # Wait for indexing

info = client.get_collection(new_collection)
print(f"\nüìä Fast Collection Stats:")
print(f"   Name: {new_collection}")
print(f"   Documents: {info.points_count:,}")
print(f"   Vector size: 768")
print(f"   Storage: In RAM (for speed)")

print("\n" + "=" * 70)
print("‚úÖ DONE! You now have a FAST collection!")
print("=" * 70)
print(f"\nTo use it, search with:")
print(f'   collection_name="{new_collection}"')
print("\nThis will give you:")
print("   ‚ö° Search speed: < 1 second")
print("   üìö Coverage: Top 100K quality documents")
print("   üíæ RAM usage: ~2-3 GB (works on 16GB!)")
print("=" * 70)
